# -*- coding: utf-8 -*-
"""Elec378HW3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1usm7TUuQJR9bhqpLYWclwLkrP14zT1nx
"""

import numpy as np
from scipy.io import loadmat
import matplotlib.pyplot as plt
import scipy.io as sc
from scipy import signal, linalg
import matplotlib
import matplotlib.image as im
import time
from sklearn.decomposition import PCA
import pandas as pd
from scipy.spatial import distance
from sklearn.preprocessing import StandardScaler
import random
import statistics
from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans

"""1. (a)"""

#get data
a = sc.loadmat('eavesdropping.mat')
data = np.squeeze(a['Y'])

#center data
means = np.mean(data, axis=0)
centered_data = data-means

#normalize data
SDs = np.std(centered_data, axis=0)
normalized_data = centered_data / SDs

# SVD
U, S, Vt = np.linalg.svd(normalized_data)

#grab the first two eigenvectors
PCs = Vt.T[:,:2]

#project data onto PCs
data_2D = np.dot(normalized_data,PCs)

#plot PCA
pc1 = data_2D[:, 0]
pc2 = data_2D[:, 1]
plt.figure(figsize=(8, 6))
plt.scatter(pc1, pc2)
plt.title('2D PCA For Eavesdropping')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

"""1. (b)"""

above_threshold = (pc1 >= 0)
below_threshold = (pc1 < 0)
plt.scatter(pc1[above_threshold], pc2[above_threshold], color='blue', label='0')
plt.scatter(pc1[below_threshold], pc2[below_threshold], color='red', label='1')
plt.title('2D PCA For Eavesdropping Color Coordinated')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

"""1. (c) Message: richb is love richb is life


"""

choices = np.empty(189,dtype = int)
for i in range(data_2D.shape[0]):
    if (data_2D[i][0] < 0):
        choices[i] = 0
    else:
        choices[i] = 1

# Reshape to 7 bit ASCII
reshaped = np.reshape(choices, (27,7))

# Binary to ASCII
message = ""
for i in reshaped:
    temp_string = ""
    for num in i:
        temp_string = temp_string + str(num)
    char = int(temp_string, 2)
    message = message + chr(char)

print(message)

"""2a. $(2^8)^3 = 16,777,216$

2b. The feature vectors represent the coloring for a single pixel. p is 3, the number of colors making up a pixel. n is the number of pixels.

2c. It's not clear how the feature vectors should be clustered, as while the colors have a few general areas, they blend together a lot.
"""

#Form the data matrix
image = im.imread('objection.png')

# Form the data matrix from the RGB image
X = image.flatten().reshape((-1,3))

# Run PCA
pca = PCA(n_components=2)

# Project pixels into 2D space
pixels_transformed = pca.fit_transform(X)

# Plot pixels in 2D space
plt.scatter(pixels_transformed[:, 0], pixels_transformed[:, 1], c=X, alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

"""2d."""

def kmeans(data,k,max_iterations = 100):
  #pick 64 random pixels
  rand_indices = np.random.choice(data.shape[0],size=k,replace=False)
  centroids = data[rand_indices,:]

  prev_centroids = np.zeros(centroids.shape)
  clusters = np.zeros(data.shape[0], dtype=int)
  changed = True

  i = 0
  #iterate 100 times or until cluster doesn't change
  while i < max_iterations and changed:
        # Vectorized assignment of clusters
        distances = cdist(data, centroids, 'euclidean')
        clusters = np.argmin(distances, axis=1)

        # update centroids
        prev_centroids = centroids.copy()
        for j in range(k):
            points_in_cluster = data[clusters == j]
            if points_in_cluster.size > 0:
                centroids[j] = np.mean(points_in_cluster, axis=0)

        # check if centroids changed
        changed = not np.array_equal(centroids, prev_centroids)
        i += 1

  return clusters, centroids

# Load the image and prepare data
image = im.imread('objection.png')
data = np.array(image).reshape((-1, 3))

# Run k-means algorithm
K = 2 ** 6
labels, centroids = kmeans(data, k=K, max_iterations=100)

kmeans_flat = np.array([centroids[label] for label in labels])

plt.imshow(kmeans_flat.reshape(image.shape))

"""SKlearn K means for comparison"""

# Running SKlearn Impl For Comparison
# SKLearn implementation of kmeans
image = im.imread('objection.png')
data = np.array(image).reshape((-1,3))

K = 2 ** 6

kmeans = KMeans(n_clusters=K, random_state = 0, n_init="auto").fit(data)

# Replace each pixel with its nearest centroid, then plot the resulting image.
labels = kmeans.labels_
centroids = kmeans.cluster_centers_
kmeans_flat = np.array([centroids[label] for label in labels])

plt.imshow(kmeans_flat.reshape(image.shape))

"""PCA scatterplot colored with the resulting Voronoi tiling"""

plt.scatter(pixels_transformed[:, 0], pixels_transformed[:, 1], c=kmeans_flat)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

"""3a.
In the PCA scatter plot, the melanoma patients' data points are observed to cluster around the center of Principal Component 1 (PC1). However, this central clustering pattern is not exclusive to melanoma patients and includes most other cancer types, making it less discriminative for melanoma diagnosis. On the other hand, along Principal Component 2 (PC2), the melanoma patients are predominantly clustered around a score of ~40. This cluster is relatively distinct and separated from the clusters of other cancer types, which are positioned lower on the PC2 axis. Therefore, PC2 appears to be more informative for a melanoma diagnosis because it provides a clearer separation between melanoma and non-melanoma patients, which could be indicative of underlying gene expression patterns that are more specific to melanoma.

"""

cancer = loadmat('cancer.mat')
X = cancer['X']
Y = cancer['Y']

# Center the data
X_centered = X - np.mean(X, axis=0)

# Apply PCA to reduce dimensionality to 2
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_centered)

# Identify melanoma patients
melanoma_patients = np.array(['MELANOMA' in str(label).upper() for label in Y.flatten()])


#Graph of melanoma or other
plt.figure(figsize=(10, 6))
for i in range(len(X_pca)):
    if melanoma_patients[i]:
        plt.scatter(X_pca[i, 0], X_pca[i, 1], color='red', label='Melanoma' if i == 0 else "")
    else:
        plt.scatter(X_pca[i, 0], X_pca[i, 1], color='blue', label='Other Cancer' if i == 0 else "")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Melanoma or Not')
plt.legend()
plt.show()

"""3b. If $v_j[k]$ is large in magnitude, this means that the genes $x_i[k]$ will be highly influential in determining the data's variance along the $v_j[k]$ principal direction. This means that $x_i[k]$ may be an important gene for differentiating one type of cancer from another.

3c. Let the most informative principal direction, the first principal component, be $v_j$. We can sort the genes based on the magnitude of the elements of $v_j$. Finally, we can plot this on a heat map to show how gene expression varies across patients.
"""

# heatmap of X after sorting columns by v_j[k]

v_j = pca.components_[1]
sorted_indices = np.argsort(v_j)[::-1]
sorted = X[:,sorted_indices]

plt.figure(figsize=(12, 8))
plt.imshow(sorted[:, :], aspect='auto', cmap='viridis')
plt.colorbar()
plt.title('Heatmap of X Sorted by Magnitude of PC1 Weights')
plt.xlabel('Sorted Genes by PC1')
plt.ylabel('Patients')
plt.show()

"""3d. Doesn't always perform better, but often contains only 2 errors, compared to PCA's 3."""

pca = PCA(n_components=6)
X_pca = pca.fit_transform(X)

kmeans = KMeans(n_clusters=14).fit(X_pca)

labels = kmeans.labels_

# For visualization
pca_2 = PCA(n_components=2)
X_pca_2 = pca_2.fit_transform(X)

cluster_colors = plt.cm.tab20(np.linspace(0, 1, 14))

plt.figure(figsize=(10, 6))
for cluster_idx in range(14):
    cluster_members = labels == cluster_idx
    plt.scatter(X_pca_2[cluster_members, 0], X_pca_2[cluster_members, 1],
                color=cluster_colors[cluster_idx], label=f'Cluster {cluster_idx}')

plt.legend()
plt.show()
