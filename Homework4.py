# -*- coding: utf-8 -*-
"""Elec378HW4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TQkVnfT0AGznRH6Liv2RMXz_vA6h9BBC
"""

#imports
import random
import numpy as np
import scipy.io as sc
from scipy import signal, linalg
import matplotlib
import matplotlib.image as im
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso

"""2. Gradient descent"""

#L(w) = 4(w_1)^2 + w_1*w_2 + 4(w_2)^2

#Gradient descent set up
alpha = 0.1
convergence = 0.0001
max_iterations = 1000

#pick random values for parameters
w1 = random.randint(0,100)
w2 = random.randint(0,100)

w1_values = [w1]
w2_values = [w2]
iterations = [0]

delta = 1
i = 0
#loop until 0 change or max iterations
while (delta > convergence and i < max_iterations):
  #find new parameters
  temp_w1 = w1 - alpha*(8*w1+w2)
  temp_w2 = w2 - alpha*(w1+8*w2)

  #update delta
  delta = np.linalg.norm(np.array([w1,w2])-np.array([temp_w1,temp_w2]))

  #update parameters
  w1 = temp_w1
  w2 = temp_w2

  w1_values.append(w1)
  w2_values.append(w2)
  iterations.append(i + 1)

  i+=1;

plt.figure(figsize=(10, 6))
plt.plot(iterations, w1_values, label='w1')
plt.plot(iterations, w2_values, label='w2')
plt.title('Gradient Descent: Parameter Estimates vs. Iteration')
plt.xlabel('Iteration')
plt.ylabel('Parameter Estimate')
plt.legend()
plt.show()

"""2. Stochastic Gradient Descent"""

#L_1(w) = 2(w_1)^2+w_1*w_2-4(w_2)^2
#L_2(w) = 3(w_1)^2+4w_1*w_2+5(w_2)^2
#L_3(w) = -(w_1)^2-4w_1*w_2+3(w_2)^2

#Gradient descent set up
alpha = 0.1
convergence = 0.0001
max_iterations = 1000

#pick random values for parameters
w1 = random.randint(0,100)
w2 = random.randint(0,100)

w1_values = [w1]
w2_values = [w2]
iterations = [0]

delta = 1
i = 0
#loop until 0 change or max iterations
while (delta > convergence and i < max_iterations):
  func = random.randint(1,3)

  #L_1
  if func == 1:
    #find new parameters
    temp_w1 = w1 - alpha*(4*w1+w2)
    temp_w2 = w2 - alpha*(w1-8*w2)

  #L_2
  if func == 2:
    #find new parameters
    temp_w1 = w1 - alpha*(6*w1+4*w2)
    temp_w2 = w2 - alpha*(4*w1+10*w2)

  #L_3
  if func == 3:
    #find new parameters
    temp_w1 = w1 - alpha*(-2*w1-4*w2)
    temp_w2 = w2 - alpha*(-4*w1+6*w2)


  #update delta
  delta = np.linalg.norm(np.array([w1,w2])-np.array([temp_w1,temp_w2]))

  #update parameters
  w1 = temp_w1
  w2 = temp_w2

  w1_values.append(w1)
  w2_values.append(w2)
  iterations.append(i + 1)

  i+=1;


plt.figure(figsize=(10, 6))
plt.plot(iterations, w1_values, label='w1')
plt.plot(iterations, w2_values, label='w2')
plt.title('Stochastic Gradient Descent: Parameter Estimates vs. Iteration')
plt.xlabel('Iteration')
plt.ylabel('Parameter Estimate')
plt.legend()
plt.show()

"""2. As for qualitative differences in the performance of the two algorithms, both run pretty quickly. However, stochastic gradient descent takes far more iterations to reach stable values for w1 and w2, with SGD taking on average ~15 to 20 and GD taking on average ~5.

4a. In this case, we are not compressively sampling because we recover $s$ from 64x64 compressive measurements, and compressive sampling needs to use fewer than that many measurements.
"""

file = sc.loadmat('CS.mat')
y = file['y']
phi = file['Phi']
psi = file['Psi']


inv = np.matmul(np.matrix.transpose(psi),np.matrix.transpose(phi))
s = np.matmul(inv,y)
x = np.matmul(psi,s)

x_image = x.reshape((64,64))
plt.imshow(x_image)

"""4b. Compressive sampling involes using $y_c$ which has fewer dimensions than the signal $s$ that we're trying to recover. By introducing bias, we can reduce the variance of the predictions, helping us prevent overfitting. Ridge regression is one potential choice for this introduction of bias, as it adds a penalty to the size of the coefficients in $s$."""

rng = np.random.default_rng()
indices = np.sort(rng.choice(len(y), int(len(y)/2), replace=False))
y_c = y[indices]
phi_c = phi[indices]

alphas = [0,0.1,1,10]
fig, axs = plt.subplots(1, len(alphas), figsize=(15,15))

for i,alpha in enumerate(alphas):
  # Run regression
  rr = Ridge(alpha=alpha)
  rr.fit((phi_c@psi),y_c)

  # Get coefficients of regression. They are s.
  s_rid = rr.coef_[0]

  # Convert wavelets to image
  x = (psi@s_rid).reshape((64,64))
  axs[i].imshow(x)
  axs[i].set_title(f'Alpha = {alpha}')
  axs[i].axis('off')

plt.show()

"""4c. As $\lambda$ increases, the image quality goes down. However, a small $\lambda$ produces a higher quality image than when $\lambda$ = 0. I ultimately prefer lasso regression, as with an appropriate $\lambda$ it is able to produce a higher quality image than ridge regression.

By driving less important signal coefficients to zero, Lasso inherently selects the variables that contribute most significantly to the signal's structure.  The sparsity promoted by Lasso aligns  with the underlying assumption of compressive sensing that the signal of interest is sparse or compressible in some basis.
"""

alphas = [0,0.0001,0.01,0.1,1]
fig, axs = plt.subplots(1, len(alphas), figsize=(15,15))

for i,alpha in enumerate(alphas):
  lasso = Lasso(alpha=alpha)
  lasso.fit((phi_c@psi), y_c)

  s_las = lasso.coef_

  x = (psi@s_las).reshape((64,64))
  axs[i].imshow(x)
  axs[i].set_title(f'Alpha = {alpha}')
  axs[i].axis('off')

plt.show()

"""4d."""

s_k = s * (np.abs(s) > 15)

# Calculate the value of K
K = np.count_nonzero(s_k)

# Convert wavelets to image
x = (psi@s_k).reshape((64,64))

plt.imshow(x)
plt.title('K-Sparse')
plt.show()

# Keep 2*K of the indices from earlier, without replacement.
sparse_indices = indices[np.sort(rng.choice(len(indices), 2*K, replace=False))]

# Get the new measurement vector, y_c_new.
# Also get the new phi_c vector, phi_c_new.

y_new = phi@psi@s_k
y_c_new = y_new[sparse_indices]

phi_c_new = phi[sparse_indices]

# Run regression
rr = Ridge(alpha = 1)
rr.fit((phi_c_new@psi), y_c_new)

# Get coefficients of regression. They are s.
s_rid2 = rr.coef_[0]

# Convert wavelets to image
x = (psi@s_rid2).reshape((64,64))
plt.imshow(x)
plt.title('Ridge, K-Sparse')

lasso = Lasso(alpha=0.0001)
lasso.fit((phi_c_new@psi), y_c_new)

# Get coefficients of regression. They are s.
s_las2 = lasso.coef_

# Convert wavelets to image
x = (psi@s_las2).reshape((64,64))
plt.imshow(x)
plt.title('LASSO, K-sparse')

"""The original signal $s$ in this problem is described as not actually sparse, meaning it contains significant information across many of its elements. When you construct $s_k$ by zeroing out elements of $s$ where $|s[n]|\leq 15$ you're enforcing artificial sparsity, potentially removing information that contributes to the accurate reconstruction of the original image. This enforced sparsity might not align with the natural sparse basis that would be ideal for compressive sensing recovery, leading to a loss of critical information."""
