# -*- coding: utf-8 -*-
"""Elec378HW6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UifbhSXDccnr4mI7WVyvrVpLy9Zh7YKq

Question 1:
"""

# importing necessary modules
import numpy as np
import random
import matplotlib.pyplot as plt
from scipy.io import loadmat
from mpl_toolkits.mplot3d import Axes3D

# Importing the data
train1 = loadmat("Train1.mat")
train2 = loadmat("Train2.mat")
# X : the data matrix of shape (n, p), Y are the labels
X1 = train1["X"]
Y1 = train1["y"]

# define parameters, n, p, lambda ,mu, T
n = X1.shape[0]
p = X1.shape[1]
lamda = 1
mu_w = 0.01
mu_b = 0.01
T = 100

# ws[i], bs[i] : the weights and intercept after ith stage of subgradient descent
#randomly initialized
w0 = np.random.rand(p)
b0 = np.random.rand()
ws = np.zeros((T+1,p))
bs = np.zeros(T+1)

# ws and bs are the length of the number of iterations. each iteration is at index iter - 1
ws[0] = w0
bs[0] = b0

def subgrad_w(X,Y,w,b):
  subgrad = np.zeros(p)
  for i in range(n):
    temp = Y[i]*(w@X[i]+b)
    if temp < 1:
      subgrad -= Y[i]*X[i]
  return subgrad


def subgrad_b(X,Y,w,b):
  subgrad = 0;
  for i in range(n):
    temp = Y[i]*(w@X[i]+b)
    if temp < 1:
      subgrad -= Y[i]
  return subgrad[0];


# plotting for 1A-E
def plot_hyperplane_2d(X, w, b, i):
  x_axis = np.linspace(np.min(X[:,0]), np.max(X[:,0]), n)
  alpha = 1
  if i == 0:
      color = 'red'  # First line
      label = 'First'
  elif i == T - 1:
      color = 'blue'  # Last line
      label = 'Last'
  else:
      color = 'black'  # Intermediate lines
      label = None  # No label
      alpha = 0.5

  slope = -w[0]/w[1]
  intercept = -b/w[0]
  plt.plot(x_axis, x_axis * slope + intercept, color=color, label=label, alpha=alpha)


#plotting for 1F-G
def plot_hyperplane_3d(X,Y,w,b,i):
  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')

  ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=Y.squeeze(), cmap='bwr')

  x0, x1 = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 50),np.linspace(X[:, 1].min(), X[:, 1].max(), 50))
  x2 = (-b - w[0] * x0 - w[1] * x1) / w[2]


  ax.plot_surface(x0, x1, x2, color='green')
  ax.set_xlabel('X1')
  ax.set_ylabel('X2')
  ax.set_zlabel('X3 (Transformed)')
  plt.show()

"""1a."""

plt.figure()
plt.scatter(X1[:,0], X1[:,1], c=Y1)
plt.ylim([np.min(X1[:, 1]) -1, np.max(X1[:,1])+1])
plot_hyperplane_2d(X1, ws[0], bs[0], 0)
plt.legend()
plt.show()

"""1b. Below are plots for a few different initializations for w and b, showing the progression of hyperplanes for $\lambda$ = 1. Now, looking at the convergence, for all the initializations of w and b, the convergence appears to be relatively fast and provide approximately the same hyperplane. While the data is not linearly separable, the plane does a reasonable job at splitting the data into 2 categories."""

def SVM(X, Y, sub_b, sub_w, dim):
  n = X.shape[0]
  p = X.shape[1]
  ws = np.zeros((T+1,p))
  bs = np.zeros(T+1)
  for k in range(3):
    w0 = np.random.rand(p)
    b0 = np.random.rand()
    ws[0] = w0
    bs[0] = b0

    if dim == 2:
      plt.figure()
      plt.scatter(X[:,0], X[:,1], c=Y)
      plt.ylim([np.min(X[:, 1]) -1, np.max(X[:,1])+1])

    i = 1
    while (i < T+1):
      if dim == 2:
        plot_hyperplane_2d(X, ws[i-1], bs[i-1], i-1)
      if dim == 3 and i==T:
        plot_hyperplane_3d(X, Y, ws[i-1], bs[i-1], i-1)

      ws[i] = ws[i-1] - mu_w*(sub_w(X,Y,ws[i-1],bs[i-1]) + 2*lamda*ws[i-1])
      bs[i] = bs[i-1] - mu_b*(sub_b(X,Y,ws[i-1],bs[i-1]))

      i = i+1

    if (dim == 2):
      plt.legend()
      plt.show()

SVM(X1, Y1, subgrad_b, subgrad_w,2)

"""1c. As $\lambda$ increases, it takes more iterations to converge to a reasonable hyperplane. However, it also includes more misclassifications. In general, a higher $\lambda$ leads to more misclassifications for the given data but less overfitting and potentially being a better predictor for the actual separation."""

lamdas = [0,0.1,10,100]
for lam in lamdas:
  lamda = lam;
  print("lambda = "+str(lamda))
  SVM(X1,Y1,subgrad_b, subgrad_w,2)

"""1d. Changes: instead of summing over all subgradients, we simply select a random i and use that subgradient."""

def stoch_subgrad_w(X,Y,w,b):
  subgrad = np.zeros(p)
  i = random.randint(0,n-1)
  temp = Y[i]*(w@X[i]+b)
  if temp < 1:
    subgrad -= Y[i]*X[i]
  return subgrad

def stoch_subgrad_b(X,Y,w,b):
  subgrad = 0;
  i = random.randint(0,n-1)
  temp = Y[i]*(w@X[i]+b)
  if temp < 1:
    subgrad -= Y[i]
  return subgrad;

lamda = 1
SVM(X1,Y1,stoch_subgrad_b,stoch_subgrad_w,2)

"""1e. This is not a good classifier because the data is not linearly separable, and thus the classifier makes many errors and does not reflect the actual trend in the data."""

X2 = train2["X"]
Y2 = train2["y"]

SVM(X2,Y2,subgrad_b,subgrad_w,2)

"""1f."""

def f(X):
  third_dim = X[:,0]**2+X[:,1]**2
  return np.column_stack((X,third_dim))

X2_3D = f(X2)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(X2_3D[:, 0], X2_3D[:, 1], X2_3D[:, 2], c=Y2.squeeze(), cmap='bwr')

ax.set_xlabel('X1')
ax.set_ylabel('X2')
ax.set_zlabel('X1^2 + X2^2')
plt.show()

"""1g. The error is much smaller than the SVM without transforming the data."""

n = X2_3D.shape[0]
p = X2_3D.shape[1]

ws = np.zeros((T+1,p))
bs = np.zeros(T+1)

SVM(X2_3D, Y2, subgrad_b, subgrad_w,3)
