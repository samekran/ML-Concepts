# -*- coding: utf-8 -*-
"""LSTM_Elec378FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UaxGDlKV5p1Otn3iIUHsOzKyhOIILbnu
"""

import librosa
import numpy as np
import csv
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import StratifiedKFold
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

from google.colab import drive
drive.mount('/content/drive')

"""**Loading Kaggle Data**"""

def extract_features(file_name, start_second, duration=3):
    try:
        # Load a segment of the audio file
        audio_timeseries, sampling_rate = librosa.load(file_name, sr=None, offset=start_second, duration=duration)

        # Extract MFCC and other features
        mfccs = librosa.feature.mfcc(y=audio_timeseries, sr=sampling_rate, n_mfcc=20)
        chroma = librosa.feature.chroma_stft(y=audio_timeseries, sr=sampling_rate)
        contrast = librosa.feature.spectral_contrast(y=audio_timeseries, sr=sampling_rate)
        tonnetz = librosa.feature.tonnetz(y=audio_timeseries, sr=sampling_rate)
        all_features = np.vstack([mfccs, chroma, contrast, tonnetz])
        features_mean = np.mean(all_features, axis=1)
        return features_mean
    except Exception as e:
        print(f"Error loading {file_name}: {e}")
        return None

# Load labels from CSV
labels = []
with open('/content/drive/My Drive/Elec378 Final Project/Dataset/train.csv', mode='r', newline='', encoding='utf-8') as file:
    reader = csv.reader(file)
    next(reader)  # Skip the header
    for row in reader:
        labels.append(row[1])

# Load dataset and extract features
features = []
labels_for_features = []

# Duration and segments for splitting
total_duration = 30
segment_duration = 3
segments_per_file = total_duration // segment_duration

for i in range(len(labels)):
    file_name = f'/content/drive/My Drive/Elec378 Final Project/Dataset/train/train{str(i).zfill(3)}.wav'
    for j in range(segments_per_file):
        start_second = j * segment_duration
        mfccs = extract_features(file_name, start_second, duration=segment_duration)
        if mfccs is not None:
            features.append(mfccs)
            # Each segment gets the same label
            labels_for_features.append(labels[i])

# Convert to numpy arrays and continue as before
features = np.array(features)
labels_for_features = np.array(labels_for_features)
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels_for_features)

# Initialize cross-validation
kf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
accuracies = []

# Scaler and model configuration
scaler = StandardScaler()  # Initialize the scaler

# Perform cross-validation
for train_index, test_index in kf.split(features, labels_encoded):
    X_train, X_test = features[train_index], features[test_index]
    y_train, y_test = labels_encoded[train_index], labels_encoded[test_index]

    # Normalize the features
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Reshape data for LSTM input (batch_size, timesteps, features)
    # Here, we use each feature vector as a timestep, so we need to add a timestep dimension
    X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])
    X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])

    # Convert integer labels to one-hot encoding
    y_train_one_hot = to_categorical(y_train)
    y_test_one_hot = to_categorical(y_test)

    # Build the LSTM model
    num_classes = y_train_one_hot.shape[1]  # Number of unique classes
    model = Sequential([
        LSTM(64, input_shape=(1, X_train_scaled.shape[2]), return_sequences=True),  # return_sequences=True if stacking LSTMs
        LSTM(64),
        Dropout(0.25),
        Dense(100, activation='relu'),
        Dropout(0.25),
        Dense(num_classes, activation='softmax')
    ])

    # Compile the model
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

    # Fit data to model
    history = model.fit(X_train_scaled, y_train_one_hot, validation_data=(X_test_scaled, y_test_one_hot), epochs=50, batch_size=32,
                        callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])

    # Evaluate the model
    _, accuracy = model.evaluate(X_test_scaled, y_test_one_hot, verbose=0)
    accuracies.append(accuracy)

# Average accuracy across all folds
average_accuracy = np.mean(accuracies)
print(f"Average Accuracy: {average_accuracy * 100:.2f}%")

"""Actual Test Data"""

test_features = []
test_file_names = []
segment_duration = 3
segments_per_file = 10

for i in range(200):  # Assuming 200 test files
    file_name = f'/content/drive/My Drive/Elec378 Final Project/Dataset/test/test{str(i).zfill(3)}.wav'
    file_features = []
    for j in range(segments_per_file):
        start_second = j * segment_duration
        mfccs = extract_features(file_name, start_second, duration=segment_duration)
        file_features.append(mfccs.reshape(1, -1))

    # Convert features of this file into a numpy array and predict
    file_features_np = np.vstack(file_features)
    file_features_scaled = scaler.transform(file_features_np)  # Normalize the features
    file_features_scaled = file_features_scaled.reshape(file_features_scaled.shape[0], 1, file_features_scaled.shape[1])  # Reshape for LSTM
    predictions = model.predict(file_features_scaled)  # Get predictions
    predicted_classes = np.argmax(predictions, axis=1)

    # Determine the most common prediction for this file using the integer labels directly
    most_common_class_index = np.bincount(predicted_classes).argmax()
    most_common_label = label_encoder.classes_[most_common_class_index]  # Translate the index back to the original label
    test_features.append((f'test{str(i).zfill(3)}.wav', most_common_label))

# Write the results to a CSV file
with open('lstm.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerow(['ID', 'Genre'])
    for file_name, prediction in test_features:
        writer.writerow([file_name, prediction])
